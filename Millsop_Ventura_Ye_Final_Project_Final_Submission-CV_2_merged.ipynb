{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_gender_bkts.csv\n",
      "train_users_2.csv\n",
      "countries.csv\n",
      "sample_submission_NDF.csv\n",
      "sessions.csv\n",
      "test_users.csv\n"
     ]
    }
   ],
   "source": [
    "# Import data files from Kaggle.\n",
    "DATA_PATH = './data/extracted'\n",
    "dfs_raw = {}\n",
    "dfs = {}\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        dfs_raw[file.split('.')[0]] = pd.read_csv(f'{DATA_PATH}/{file}')\n",
    "        dfs = dfs_raw.copy()\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDF      124543\n",
      "US        62376\n",
      "other     10094\n",
      "FR         5023\n",
      "IT         2835\n",
      "GB         2324\n",
      "ES         2249\n",
      "CA         1428\n",
      "DE         1061\n",
      "NL          762\n",
      "AU          539\n",
      "PT          217\n",
      "Name: country_destination, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split training dataset into data and labels.\n",
    "train_data_all = dfs[\"train_users_2\"]\n",
    "train_labels_all = dfs[\"train_users_2\"].iloc[:, -1:]\n",
    "\n",
    "# Evaluate existing representation of classes.\n",
    "print(pd.value_counts(train_labels_all['country_destination']))\n",
    "countries = train_labels_all['country_destination'].unique()\n",
    "\n",
    "# Create pd for each country.\n",
    "train_data_country = {}\n",
    "train_labels_country = {}\n",
    "min_count = -1\n",
    "for country in countries:\n",
    "    train_data_country[country] = train_data_all.loc[train_labels_all['country_destination'] == country]\n",
    "    train_labels_country[country] = train_labels_all.loc[train_labels_all['country_destination'] == country]\n",
    "    count = train_labels_country[country].shape[0]\n",
    "    if (min_count == -1 or count < min_count):\n",
    "        min_count = count\n",
    "\n",
    "# Create balanced training dataset.\n",
    "balanced_train_data = pd.DataFrame(columns=train_data_all.columns.values)\n",
    "for country in countries:\n",
    "    country_pd = train_data_country[country].sample(n=min_count, random_state=1)\n",
    "    balanced_train_data = pd.concat([balanced_train_data, country_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into data and labels (panda dataframes).\n",
    "#reduced this to 10k / 213k since it was taking forever to even test anything\n",
    "#train_data   = dfs[\"train_users_2\"][:10000].iloc[:, 0:-1] #we should randomize since accounts are in chronological order\n",
    "#train_labels = dfs[\"train_users_2\"][:10000][\"country_destination\"].ravel()\n",
    "\n",
    "# Set train/dev split to 0.04685/0.95315 to give train size of 10k.  0.04685 = 10000/213451\n",
    "test_size = 0.95315\n",
    "\n",
    "# Use (train_test_split) to randomize train_users_2 before splitting into train/dev.\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(dfs[\"train_users_2\"].iloc[:, 0:-1], dfs[\"train_users_2\"].iloc[:, -1:], test_size=test_size, random_state=42)\n",
    "\n",
    "# Final test data for Kaggle submission.\n",
    "test_data = dfs[\"test_users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to bucket ages prior to one-hot encoding\n",
    "def age_bucketer(df_input):\n",
    "    df = df_input\n",
    "    df.loc[(pd.isnull(df.age), 'age_bucket')] = 'unknown'\n",
    "    df.loc[(pd.notnull(df.age), 'age_bucket')] = pd.cut(df['age'],\n",
    "                                                        [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94,99,10000],\n",
    "                                                        labels=['0-4', '5-9', '10-14','15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49',\n",
    "                                                                '50-54', '55-59','60-64', '65-69','70-74','75-79','80-84','85-89','90-94','95-99','100+'],\n",
    "                                                        include_lowest=True)\n",
    "    return df.drop(['age'], axis=1)\n",
    "\n",
    "#Since NaN's in categorical data will cause issues with our pipeline we will replace that with \"unknown\".\n",
    "def clean_first_affiliate_tracked_nulls(df_input):\n",
    "    df_input['first_affiliate_tracked'] = df_input['first_affiliate_tracked'].fillna(\"unknown\", inplace=False)\n",
    "    return df_input\n",
    "\n",
    "#Add month and year features\n",
    "def feature_creator (df_input):\n",
    "    df = df_input\n",
    "    df['first_active_date'] = pd.to_datetime(df.timestamp_first_active,format='%Y%m%d%H%M%S')\n",
    "    df['year_first_active'] = df['first_active_date'].dt.year\n",
    "    df['month_first_active'] = df['first_active_date'].dt.month\n",
    "    df['season'] = ''\n",
    "    df.loc[(df['month_first_active'].isin([12, 1, 2]), 'season')] = 'Winter'\n",
    "    df.loc[(df['month_first_active'].isin([3, 4, 5]), 'season')] = 'Spring'\n",
    "    df.loc[(df['month_first_active'].isin([6, 7, 8]), 'season')] = 'Summer'\n",
    "    df.loc[(df['month_first_active'].isin([9, 10, 11]), 'season')] = 'Fall'\n",
    "    return df.drop(['first_active_date'], axis=1) #consider dropping month as well\n",
    "\n",
    "def session_feature_creator(df_input):\n",
    "    df = df_input\n",
    "    session_agg = dfs['sessions'].groupby('user_id').agg({\"secs_elapsed\": np.sum, \"device_type\": pd.Series.nunique, 'action': 'count'}).reset_index(\n",
    "        ).rename(columns={'secs_elapsed':'total_time', 'device_type':'unique_device_types', 'action': 'unique_actions'})\n",
    "    return df.merge(session_agg, left_on='id', right_on='user_id', how='left')\n",
    "\n",
    "def nan_destroyer(df_input):\n",
    "    # funcation to remove nans from numerical fields, need to determine strategy\n",
    "    return none\n",
    "\n",
    "def feautre_selector(df_input):\n",
    "    # final function to remove IDs, repetitive features, and any features we do not want to be passed to our models\n",
    "    return none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preprocessor pipeline.\n",
    "def create_preprocessor_pipeline():\n",
    "\n",
    "    column_transformer = make_column_transformer(\n",
    "        (['gender',\n",
    "          'signup_method',\n",
    "          'signup_flow',\n",
    "          'language',\n",
    "          'affiliate_channel',\n",
    "          'affiliate_provider',\n",
    "          'first_affiliate_tracked',\n",
    "          'signup_app',\n",
    "          'first_device_type',\n",
    "          'first_browser',\n",
    "          'age_bucket',\n",
    "          'season'\n",
    "         ], OneHotEncoder(handle_unknown='ignore')),remainder='drop') # when we add in sessions features we will want to pass remainders\n",
    "    \n",
    "    preprocessor = make_pipeline(\n",
    "        FunctionTransformer(age_bucketer, validate=False),\n",
    "        FunctionTransformer(feature_creator, validate=False),\n",
    "        FunctionTransformer(clean_first_affiliate_tracked_nulls, validate=False),\n",
    "        column_transformer)\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stage: Data Preprocessor.\n",
    "preprocessor = create_preprocessor_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = balanced_train_data.iloc[:, 0:-1]\n",
    "balanced_labels = balanced_train_data.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate predictions of destination countries, we examined a variety of machine learning algorithms.  In this next section, we will examine the efficacy of k-neighbors, Bernoulli Naive Bayes, Random Forest, Logistic Regression, and XGBoost.  While we will go into detail on each specific model, these models were selected due to their efficacy at solving customer purchase prediction problems or their efficacy at classifying imbalanced classes in available research and literature.  Given that our task is ultimately predicting customer behavior in an environment in which two choices are selected much more frequently than all others, we believe these choices to be appropriate for our use case.  Ultimately, inspired by both previous winners of this competition as well as additional research, we will combine our most effective and practical (evaluated by runtime) models into an ensemble voting classifier to generate the most robust predictions possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout our process, we will compare accuracy scores and class-level F1 scores while optimizing our models to the optimal parameters for both \"Weighted F1\" scores and \"Macro F1\" scores.  Optimizing for \"Weighted F1\" scores will tune our models to a weighted average F1 score across all classes.  Given that our data contains significantly more examples of the \"NDF\" and \"US\" classes, the F1 scores for those two classes will largely dictate which parameters are deemed optimal.  Optimizing for \"Macro F1\" will maximize a non-weighted F1 score, meaning that the F1 score for each class is treated equally.  This typically results in uncommon classes being predicted more frequently, often at the expense of overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of our dataset, we will most often be using a 10,000 record random sample from the training data for training, while testing on a much larger \"development\" set consisting of the remainder of the training set.  At times, we will train with a \"balanced\" training set consisting of an undersampled dataset with equal number of records for each class.  While this often significantly lowers the accuracy of each model, it does provide a helpful point of comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we start with a k-neighbors classifier.  In their paper *KNN Approach to Unbalanced Data Distributions*, Zhang and Mati demonstrate the effectiveness of a k-neighbors classifier at learning and accurately predicting minority classes without reducing the ability to predict common clases.  Zhang and Mati also discuss the downsides of using other types of classifiers, particularly decision tree classifiers, when classes are heavily inbalanced. We test our model using 1, 3, 5, and 7 neighbors to see which is most effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted F1 Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5705599874171177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00       249\n",
      "          CA       0.01      0.01      0.01       780\n",
      "          DE       0.00      0.01      0.01       441\n",
      "          ES       0.01      0.01      0.01      1127\n",
      "          FR       0.01      0.03      0.02      1888\n",
      "          GB       0.01      0.02      0.01       983\n",
      "          IT       0.01      0.02      0.01       991\n",
      "         NDF       0.82      0.64      0.72    151934\n",
      "          NL       0.00      0.00      0.00        73\n",
      "          PT       0.00      0.00      0.00         7\n",
      "          US       0.30      0.42      0.35     42554\n",
      "       other       0.02      0.06      0.03      2424\n",
      "\n",
      "   micro avg       0.57      0.57      0.57    203451\n",
      "   macro avg       0.10      0.10      0.10    203451\n",
      "weighted avg       0.68      0.57      0.61    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_neighbors':[1,3,5,7], 'weights':['distance']}\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_gs = GridSearchCV(knn, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, knn_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 7, 'weights': 'distance'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro F1 Optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5246767034814279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00       733\n",
      "          CA       0.01      0.01      0.01      1820\n",
      "          DE       0.01      0.01      0.01      1682\n",
      "          ES       0.02      0.01      0.01      2752\n",
      "          FR       0.04      0.03      0.03      5885\n",
      "          GB       0.02      0.02      0.02      2381\n",
      "          IT       0.02      0.01      0.02      4247\n",
      "         NDF       0.75      0.65      0.69    136099\n",
      "          NL       0.00      0.01      0.00       111\n",
      "          PT       0.00      0.00      0.00        33\n",
      "          US       0.30      0.40      0.34     44521\n",
      "       other       0.02      0.06      0.03      3187\n",
      "\n",
      "   micro avg       0.52      0.52      0.52    203451\n",
      "   macro avg       0.10      0.10      0.10    203451\n",
      "weighted avg       0.57      0.52      0.54    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_neighbors':[1,3,5,7], 'weights':['distance']}\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_gs = GridSearchCV(knn, params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, knn_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3, 'weights': 'distance'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's performance, as well as the optimum number of neighbors, varies with both scoring method.  Optimizing the model around a weighted F1 score results in significantly higher accuracy (XXXX) than the optimizing for a macro F1 score, but at the cost of lower precision and recall for minority classes.  In addition, the weighted F1-optimized model performs best when trained with 7 neighbors, while the macro F1-optimized model performs best with only three neighbors.  This makes intuitive sense: given the inbalance in the data, the inclusion of more neighbors makes the model more likely to predict the common \"NDF\" and \"US\" classes, while fewer neighbors results in a highler likelihood of predicting uncommon classes.  Regardless, the F1 scores for minority classes are still low even when optimizing around macro F1 scores.  In an attempt to improve F1 for minority classes, we will train the models again, this time using the balanced dataset while optimizing for macro F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Dataset F1 Macro Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.13098977149289018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.49      0.01      0.02     30566\n",
      "          CA       0.23      0.02      0.03     20785\n",
      "          DE       0.27      0.02      0.03     15151\n",
      "          ES       0.17      0.02      0.04     17593\n",
      "          FR       0.12      0.03      0.05     17747\n",
      "          GB       0.16      0.02      0.04     16653\n",
      "          IT       0.13      0.03      0.05     12338\n",
      "         NDF       0.16      0.75      0.26     24889\n",
      "          NL       0.26      0.02      0.03     12000\n",
      "          PT       0.72      0.01      0.03     10144\n",
      "          US       0.07      0.30      0.12     14740\n",
      "       other       0.07      0.06      0.07     10845\n",
      "\n",
      "   micro avg       0.13      0.13      0.13    203451\n",
      "   macro avg       0.24      0.11      0.06    203451\n",
      "weighted avg       0.24      0.13      0.07    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_neighbors':[1,3,5,7], 'weights':['distance']}\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_gs = GridSearchCV(knn, params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, knn_gs)\n",
    "pipeline.fit(balanced_data, balanced_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3, 'weights': 'distance'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this does significantly improve precision for a number of minority classes, recall is largely unchanged resulting in only moreate improvements to F1 for uncommon classes, as well as a drop in overall F1.  We will explore this further in the remaining model training, but this could possibly indicate a lack of feature distinction between the uncommon and common classes, making it difficult for most models to accurately predict the uncommon classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free some memory\n",
    "del knn, knn_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now examine the efficacy of a Bernoulli Naive Bayes model.  Our rationale for choosing this model is do to the quantity of binary features in our post-pipeline dataset.  Given the high number of categorical features, which are one-hot encoded during our pipeline process, we suspect Bernoulli Naive Bayes may perform effectively.  As with KNN, we will examine the efficacy of optimizing the model for both weighted and macro F1 scores, as well as optimizing on the balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted F1 Optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5676698566239536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00        16\n",
      "          CA       0.00      0.05      0.00        22\n",
      "          DE       0.00      0.00      0.00       144\n",
      "          ES       0.00      0.01      0.00       147\n",
      "          FR       0.00      0.02      0.00        58\n",
      "          GB       0.00      0.00      0.00        83\n",
      "          IT       0.00      0.03      0.00       152\n",
      "         NDF       0.67      0.70      0.69    113654\n",
      "          NL       0.00      0.00      0.00        18\n",
      "          PT       0.00      0.00      0.00         7\n",
      "          US       0.60      0.40      0.48     88602\n",
      "       other       0.00      0.05      0.01       548\n",
      "\n",
      "   micro avg       0.57      0.57      0.57    203451\n",
      "   macro avg       0.11      0.10      0.10    203451\n",
      "weighted avg       0.64      0.57      0.59    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'alpha': [1.05, .1]}\n",
    "bnb = BernoulliNB()\n",
    "bnb_gs = GridSearchCV(bnb, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, bnb_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.05}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro F1 Optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5685644209170758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00        74\n",
      "          CA       0.00      0.00      0.00        35\n",
      "          DE       0.00      0.00      0.00        48\n",
      "          ES       0.00      0.04      0.00        85\n",
      "          FR       0.00      0.08      0.00        25\n",
      "          GB       0.00      0.00      0.00        66\n",
      "          IT       0.00      0.07      0.01       122\n",
      "         NDF       0.67      0.70      0.69    113902\n",
      "          NL       0.00      0.02      0.00        66\n",
      "          PT       0.00      0.00      0.00        71\n",
      "          US       0.60      0.40      0.48     88530\n",
      "       other       0.00      0.06      0.01       427\n",
      "\n",
      "   micro avg       0.57      0.57      0.57    203451\n",
      "   macro avg       0.11      0.11      0.10    203451\n",
      "weighted avg       0.64      0.57      0.59    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'alpha': [1.05, .1]}\n",
    "bnb = BernoulliNB()\n",
    "bnb_gs = GridSearchCV(bnb, params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, bnb_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the Laplace smoothing parameters (alpha) for the weighted and macro F1 optimized model versions, we see that the macro F1-optimized model performs best under a significantly smaller (0.1) alpha, corresponding to stronger smoothing.  However, in spite of the difference in optimized tuning parameters, both models perform very similarly.  In comparison to the KNN models, both models show improvement at predicting \"US\" classes at the expense of predicting \"NDF\" classes.  Still, both models perform rather poorly when predicting uncommon classes.  Once again, we will examine if balancing the dataset has any effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Macro F1 Optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.27802271800089456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.08      0.00      0.01     11274\n",
      "          CA       0.09      0.01      0.02      8145\n",
      "          DE       0.11      0.01      0.02     11096\n",
      "          ES       0.13      0.02      0.03     17514\n",
      "          FR       0.14      0.03      0.05     19721\n",
      "          GB       0.15      0.01      0.03     23456\n",
      "          IT       0.04      0.02      0.02      7165\n",
      "         NDF       0.42      0.76      0.54     66450\n",
      "          NL       0.15      0.01      0.01     15820\n",
      "          PT       0.06      0.00      0.01      4880\n",
      "          US       0.06      0.40      0.11      9563\n",
      "       other       0.05      0.06      0.06      8367\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    203451\n",
      "   macro avg       0.12      0.11      0.08    203451\n",
      "weighted avg       0.21      0.28      0.20    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'alpha': [1.05, .1]}\n",
    "bnb = BernoulliNB()\n",
    "bnb_gs = GridSearchCV(bnb, params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, bnb_gs)\n",
    "pipeline.fit(balanced_data, balanced_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with KNN, we see a slight improvement in F1 scores for some uncommon classes at the expense of overall F1 scores.  Furthermore, this model experiences a significant drop in prediction accuracy, with the model predicting several uncommon classes with higher frequency than \"US\" class predictions.  Given that all Naive Bayes models depend on priors derived from the frequency of classes found in the data, balancing the dataset results in significant overprediction of minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forst Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Zhang and Mati point out some of the drawbacks of decision tree classifiers at predicting inblanced classes, Random Forest Classifiers have been shown to be effective at predicting inbalanced classes in some cases (CITATION NEEDED).  Similar to KNN, Random Forest classifiers predict non-linear decision boundaries, and therefore may prove effective, especially when combined with linear classifiers.  Below, we will optimize two models, one using SKlearn's built-in class balancing feature and one without any additional class balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.46626460425360405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.01      0.00      0.00      2450\n",
      "          CA       0.02      0.01      0.01      6287\n",
      "          DE       0.01      0.01      0.01      1803\n",
      "          ES       0.02      0.01      0.01      5363\n",
      "          FR       0.03      0.02      0.03      5753\n",
      "          GB       0.02      0.02      0.02      2979\n",
      "          IT       0.03      0.01      0.02      6003\n",
      "         NDF       0.60      0.67      0.63    106921\n",
      "          NL       0.01      0.00      0.01      2961\n",
      "          PT       0.00      0.00      0.00       984\n",
      "          US       0.38      0.43      0.40     53058\n",
      "       other       0.05      0.06      0.06      8889\n",
      "\n",
      "   micro avg       0.47      0.47      0.47    203451\n",
      "   macro avg       0.10      0.10      0.10    203451\n",
      "weighted avg       0.42      0.47      0.44    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_estimators':[300], 'max_depth':[5,10,20]}\n",
    "rf = RandomForestClassifier(n_jobs = -1, class_weight = 'balanced_subsample')\n",
    "rf_gs = GridSearchCV(rf, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, rf_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20, 'n_estimators': 300}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Class Balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6115624892480254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00        35\n",
      "          CA       0.00      0.02      0.00        55\n",
      "          DE       0.00      0.03      0.00        39\n",
      "          ES       0.00      0.01      0.00        70\n",
      "          FR       0.00      0.04      0.00       198\n",
      "          GB       0.00      0.01      0.00       101\n",
      "          IT       0.00      0.03      0.00       117\n",
      "         NDF       0.84      0.67      0.74    149064\n",
      "          NL       0.00      0.00      0.00        24\n",
      "          PT       0.00      0.00      0.00         5\n",
      "          US       0.41      0.47      0.44     52878\n",
      "       other       0.01      0.07      0.01       865\n",
      "\n",
      "   micro avg       0.61      0.61      0.61    203451\n",
      "   macro avg       0.11      0.11      0.10    203451\n",
      "weighted avg       0.72      0.61      0.66    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_estimators':[300], 'max_depth':[5,10,20]}\n",
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "rf_gs = GridSearchCV(rf, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, rf_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20, 'n_estimators': 300}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the class-balanced model has slighly higher uncommon class F1 scores than the model without balancing.  However, the model without balancing has the highest accuracy and F1 scores our of all of our models so far.  Both models optimize around the same number of estimators an tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free memory\n",
    "del rf, rf_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous section, we will now optimize two logistic regression models: one with class balancing applied and one without.  We will do so while optimizing the regulariztion paramaters, identifying the optimum value of 'C' for both l1 and l2 penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5527964964536916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.07      0.00      0.01      7144\n",
      "          CA       0.03      0.01      0.02      3861\n",
      "          DE       0.01      0.00      0.00      1755\n",
      "          ES       0.03      0.02      0.02      2750\n",
      "          FR       0.00      0.02      0.00        92\n",
      "          GB       0.01      0.01      0.01       890\n",
      "          IT       0.01      0.02      0.01       819\n",
      "         NDF       0.78      0.69      0.74    134627\n",
      "          NL       0.03      0.01      0.01      3667\n",
      "          PT       0.03      0.00      0.00      7473\n",
      "          US       0.32      0.47      0.38     40287\n",
      "       other       0.00      0.15      0.00        86\n",
      "\n",
      "   micro avg       0.55      0.55      0.55    203451\n",
      "   macro avg       0.11      0.12      0.10    203451\n",
      "weighted avg       0.59      0.55      0.56    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'C':[.001,.01, .1, 1, 10, 100, .007], 'penalty':['l1', 'l2']}\n",
    "lr = LogisticRegression(class_weight = 'balanced', tol=.01)\n",
    "lr_gs = GridSearchCV(lr, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, lr_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.007, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Class Balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6304171520415235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00         0\n",
      "          CA       0.00      0.00      0.00         0\n",
      "          DE       0.00      0.00      0.00         0\n",
      "          ES       0.00      0.00      0.00         0\n",
      "          FR       0.00      0.00      0.00         0\n",
      "          GB       0.00      0.00      0.00         0\n",
      "          IT       0.00      0.00      0.00         0\n",
      "         NDF       0.86      0.68      0.76    151099\n",
      "          NL       0.00      0.00      0.00         0\n",
      "          PT       0.00      0.00      0.00         0\n",
      "          US       0.43      0.49      0.46     52352\n",
      "       other       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.63      0.63      0.63    203451\n",
      "   macro avg       0.11      0.10      0.10    203451\n",
      "weighted avg       0.75      0.63      0.68    203451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "params={'C':[.001,.01, .1, 1, 10, 100, .007], 'penalty':['l1', 'l2']}\n",
    "lr = LogisticRegression(tol=.01)\n",
    "lr_gs = GridSearchCV(lr, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, lr_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While each model optimizes around different penalty and penalty types, the non-class balanced model retuns zero predictions for all classes other than \"NDF\" and \"US\".  (Add additional commentary, show class level prediction properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final individual model we revisit decision tree-based models with XGBoost.  Our motivation for this is largely due to the success of this algorithm with other users on Kaggle.  In their paper *XGBoost: A Scalable Tree Boosting System*, Chen and Guestrin describe the flexibility of the model, as well as its ability to scale quickly to larger datasets.  While XGBoost may not prove the most effective model for the purposes of this competition, it may provide a scalable alternative for even larger datasets or a substitute in a production system.  Thus, we think it is worth pursuing. As with some of our previous examples, we will first optimize a model using the full training set before using a balanced class dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Training Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6335333815021799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00         0\n",
      "          CA       0.00      0.00      0.00         0\n",
      "          DE       0.00      0.00      0.00         0\n",
      "          ES       0.00      0.00      0.00         0\n",
      "          FR       0.00      0.00      0.00         0\n",
      "          GB       0.00      0.00      0.00         0\n",
      "          IT       0.00      0.00      0.00         0\n",
      "         NDF       0.85      0.69      0.76    146802\n",
      "          NL       0.00      0.00      0.00         0\n",
      "          PT       0.00      0.00      0.00         0\n",
      "          US       0.47      0.49      0.48     56649\n",
      "       other       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.63      0.63      0.63    203451\n",
      "   macro avg       0.11      0.10      0.10    203451\n",
      "weighted avg       0.74      0.63      0.68    203451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#params={'reg_lambda':[100]}\n",
    "params={'booster':['gbtree', 'gblinear','dart']}\n",
    "xgb = XGBClassifier(nthread=-1)\n",
    "xgb_gs = GridSearchCV(xgb, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, xgb_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Training Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.31606627640070584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.12      0.01      0.01      9385\n",
      "          CA       0.16      0.01      0.02     16962\n",
      "          DE       0.12      0.01      0.02     10345\n",
      "          ES       0.10      0.02      0.03     12986\n",
      "          FR       0.13      0.04      0.06     16613\n",
      "          GB       0.09      0.02      0.03     11602\n",
      "          IT       0.10      0.02      0.03     14100\n",
      "         NDF       0.47      0.77      0.58     71539\n",
      "          NL       0.15      0.01      0.02     11281\n",
      "          PT       0.13      0.00      0.01      6736\n",
      "          US       0.12      0.43      0.18     16001\n",
      "       other       0.03      0.06      0.04      5901\n",
      "\n",
      "   micro avg       0.32      0.32      0.32    203451\n",
      "   macro avg       0.14      0.12      0.09    203451\n",
      "weighted avg       0.24      0.32      0.24    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#params={'reg_lambda':[100]}\n",
    "params={'booster':['gbtree', 'gblinear','dart']}\n",
    "xgb = XGBClassifier(nthread=-1)\n",
    "xgb_gs = GridSearchCV(xgb, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, xgb_gs)\n",
    "pipeline.fit(balanced_data, balanced_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to logistic regression, the non-class balanced trained model only returns predictions of \"NDF\" or \"US\", but does so with a relatively high accuracy.  The balanced model returns more predictions for the uncommon classes, but at a cost of overall accuracy and F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing individual models, we will attempt to improve our predictions by combining classifiers through a \"Voting Classifier\".  This classifier will utilize predictions from multiple individual classifiers to pick a single prediction for each instance.  When set to \"hard\" voting each classifier has a single vote, while setting the voting to \"soft\" will generate predictions based on weighted averages derived from the strength(probability) of the prediction from each individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing a variety of settings and model combinations fails to yield better accuracy or F1 scores than some of our individual classifiers.  This suggests that most of our models are optimizing around the same features, so additional models are not learning additional information.  As a result, combining models only results in incremental gains over some of our individual models, and slight decreases over others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6169151294414872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00         0\n",
      "          CA       0.00      0.00      0.00         0\n",
      "          DE       0.00      0.00      0.00         0\n",
      "          ES       0.00      0.00      0.00         0\n",
      "          FR       0.00      0.00      0.00         9\n",
      "          GB       0.00      0.00      0.00         0\n",
      "          IT       0.00      0.00      0.00         2\n",
      "         NDF       0.81      0.69      0.74    138558\n",
      "          NL       0.00      0.00      0.00         1\n",
      "          PT       0.00      0.00      0.00         0\n",
      "          US       0.50      0.46      0.48     64834\n",
      "       other       0.00      0.09      0.00        47\n",
      "\n",
      "   micro avg       0.62      0.62      0.62    203451\n",
      "   macro avg       0.11      0.10      0.10    203451\n",
      "weighted avg       0.71      0.62      0.66    203451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB(alpha=1.05)\n",
    "rf = RandomForestClassifier(n_jobs = -1, n_estimators=300, max_depth=20, class_weight='balanced')\n",
    "lr = LogisticRegression()#C=.007, class_weight='balanced')\n",
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "lr_2 = LogisticRegression(C=.007, class_weight='balanced')\n",
    "rf_2 = RandomForestClassifier(n_jobs = -1, n_estimators=300, max_depth=20)\n",
    "\n",
    "vc = VotingClassifier(estimators = [('lr', lr),\n",
    "                                    ('lr_2', lr_2),\n",
    "                                    ('rf', rf),\n",
    "                                    ('rf_2', rf_2)], voting='soft')\n",
    "\n",
    "pipeline = make_pipeline(preprocessor, vc)\n",
    "\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "bnb = BernoulliNB(alpha=1.05)\n",
    "rf = RandomForestClassifier(n_jobs = -1, n_estimators=300, max_depth=10, class_weight='balanced')\n",
    "lr = LogisticRegression()#C=.007, class_weight='balanced')\n",
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "vc = VotingClassifier(estimators = [('bnb', bnb),\n",
    "                                    ('rf', rf),\n",
    "                                    ('lr', lr),\n",
    "                                    ('xgb', xgb)], voting='hard')\n",
    "\n",
    "pipeline = make_pipeline(preprocessor, vc)\n",
    "\n",
    "#final_model = cross_validate(pipeline, train_data, train_labels,\n",
    "#                      scoring=[\"f1_weighted\"],\n",
    "#                      return_train_score=True, cv=3, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Score: 0.6306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <th>train_f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.050808</td>\n",
       "      <td>1.058576</td>\n",
       "      <td>0.573121</td>\n",
       "      <td>0.603624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.908152</td>\n",
       "      <td>1.053457</td>\n",
       "      <td>0.585729</td>\n",
       "      <td>0.596188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.193894</td>\n",
       "      <td>0.947466</td>\n",
       "      <td>0.574207</td>\n",
       "      <td>0.600389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_f1_weighted  train_f1_weighted\n",
       "0  6.050808    1.058576          0.573121           0.603624\n",
       "1  5.908152    1.053457          0.585729           0.596188\n",
       "2  6.193894    0.947466          0.574207           0.600389"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show accuracy results.\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "score = pipeline.score(dev_data, dev_labels.values.ravel())\n",
    "print(\"Pipeline Score: %.4f\" %(score))\n",
    "display(pd.DataFrame(final_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6209210080068419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00         0\n",
      "          CA       0.00      0.00      0.00         0\n",
      "          DE       0.00      0.00      0.00         0\n",
      "          ES       0.00      0.00      0.00         0\n",
      "          FR       0.00      0.00      0.00         0\n",
      "          GB       0.00      0.00      0.00         0\n",
      "          IT       0.00      0.00      0.00         0\n",
      "         NDF       0.81      0.70      0.75    137804\n",
      "          NL       0.00      0.00      0.00         0\n",
      "          PT       0.00      0.00      0.00         0\n",
      "          US       0.51      0.46      0.49     65639\n",
      "       other       0.00      0.12      0.00         8\n",
      "\n",
      "   micro avg       0.62      0.62      0.62    203451\n",
      "   macro avg       0.11      0.11      0.10    203451\n",
      "weighted avg       0.71      0.62      0.66    203451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRITTEN: kaggle_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for test data to submit to Kaggle for scoring.\n",
    "predictions = pipeline.predict(test_data)\n",
    "\n",
    "# Save to csv\n",
    "final_csv = 'kaggle_submission.csv'\n",
    "predictions_pd = pd.DataFrame(data=predictions, columns=['country'])\n",
    "test_result = pd.concat([test_data['id'], predictions_pd], axis=1, sort=False)\n",
    "test_result.to_csv(final_csv, index=False)\n",
    "print(\"WRITTEN: %s\" %(final_csv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
