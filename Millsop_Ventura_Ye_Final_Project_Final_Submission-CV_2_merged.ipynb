{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_gender_bkts.csv\n",
      "train_users_2.csv\n",
      "countries.csv\n",
      "sample_submission_NDF.csv\n",
      "sessions.csv\n",
      "test_users.csv\n"
     ]
    }
   ],
   "source": [
    "# Import data files from Kaggle.\n",
    "DATA_PATH = './data/extracted'\n",
    "dfs_raw = {}\n",
    "dfs = {}\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        dfs_raw[file.split('.')[0]] = pd.read_csv(f'{DATA_PATH}/{file}')\n",
    "        dfs = dfs_raw.copy()\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDF      124543\n",
      "US        62376\n",
      "other     10094\n",
      "FR         5023\n",
      "IT         2835\n",
      "GB         2324\n",
      "ES         2249\n",
      "CA         1428\n",
      "DE         1061\n",
      "NL          762\n",
      "AU          539\n",
      "PT          217\n",
      "Name: country_destination, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split training dataset into data and labels.\n",
    "train_data_all = dfs[\"train_users_2\"]\n",
    "train_labels_all = dfs[\"train_users_2\"].iloc[:, -1:]\n",
    "\n",
    "# Evaluate existing representation of classes.\n",
    "print(pd.value_counts(train_labels_all['country_destination']))\n",
    "countries = train_labels_all['country_destination'].unique()\n",
    "\n",
    "# Create pd for each country.\n",
    "train_data_country = {}\n",
    "train_labels_country = {}\n",
    "min_count = -1\n",
    "for country in countries:\n",
    "    train_data_country[country] = train_data_all.loc[train_labels_all['country_destination'] == country]\n",
    "    train_labels_country[country] = train_labels_all.loc[train_labels_all['country_destination'] == country]\n",
    "    count = train_labels_country[country].shape[0]\n",
    "    if (min_count == -1 or count < min_count):\n",
    "        min_count = count\n",
    "\n",
    "# Create balanced training dataset.\n",
    "balanced_train_data = pd.DataFrame(columns=train_data_all.columns.values)\n",
    "for country in countries:\n",
    "    country_pd = train_data_country[country].sample(n=min_count, random_state=1)\n",
    "    balanced_train_data = pd.concat([balanced_train_data, country_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into data and labels (panda dataframes).\n",
    "#reduced this to 10k / 213k since it was taking forever to even test anything\n",
    "#train_data   = dfs[\"train_users_2\"][:10000].iloc[:, 0:-1] #we should randomize since accounts are in chronological order\n",
    "#train_labels = dfs[\"train_users_2\"][:10000][\"country_destination\"].ravel()\n",
    "\n",
    "# Set train/dev split to 0.04685/0.95315 to give train size of 10k.  0.04685 = 10000/213451\n",
    "test_size = 0.95315\n",
    "\n",
    "# Use (train_test_split) to randomize train_users_2 before splitting into train/dev.\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(dfs[\"train_users_2\"].iloc[:, 0:-1], dfs[\"train_users_2\"].iloc[:, -1:], test_size=test_size, random_state=42)\n",
    "\n",
    "# Final test data for Kaggle submission.\n",
    "test_data = dfs[\"test_users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to bucket ages prior to one-hot encoding\n",
    "def age_bucketer(df_input):\n",
    "    df = df_input\n",
    "    df.loc[(pd.isnull(df.age), 'age_bucket')] = 'unknown'\n",
    "    df.loc[(pd.notnull(df.age), 'age_bucket')] = pd.cut(df['age'],\n",
    "                                                        [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94,99,10000],\n",
    "                                                        labels=['0-4', '5-9', '10-14','15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49',\n",
    "                                                                '50-54', '55-59','60-64', '65-69','70-74','75-79','80-84','85-89','90-94','95-99','100+'],\n",
    "                                                        include_lowest=True)\n",
    "    return df.drop(['age'], axis=1)\n",
    "\n",
    "#Since NaN's in categorical data will cause issues with our pipeline we will replace that with \"unknown\".\n",
    "def clean_first_affiliate_tracked_nulls(df_input):\n",
    "    df_input['first_affiliate_tracked'] = df_input['first_affiliate_tracked'].fillna(\"unknown\", inplace=False)\n",
    "    return df_input\n",
    "\n",
    "#Add month and year features\n",
    "def feature_creator (df_input):\n",
    "    df = df_input\n",
    "    df['first_active_date'] = pd.to_datetime(df.timestamp_first_active,format='%Y%m%d%H%M%S')\n",
    "    df['year_first_active'] = df['first_active_date'].dt.year\n",
    "    df['month_first_active'] = df['first_active_date'].dt.month\n",
    "    df['season'] = ''\n",
    "    df.loc[(df['month_first_active'].isin([12, 1, 2]), 'season')] = 'Winter'\n",
    "    df.loc[(df['month_first_active'].isin([3, 4, 5]), 'season')] = 'Spring'\n",
    "    df.loc[(df['month_first_active'].isin([6, 7, 8]), 'season')] = 'Summer'\n",
    "    df.loc[(df['month_first_active'].isin([9, 10, 11]), 'season')] = 'Fall'\n",
    "    return df.drop(['first_active_date'], axis=1) #consider dropping month as well\n",
    "\n",
    "def session_feature_creator(df_input):\n",
    "    df = df_input\n",
    "    session_agg = dfs['sessions'].groupby('user_id').agg({\"secs_elapsed\": np.sum, \"device_type\": pd.Series.nunique, 'action': 'count'}).reset_index(\n",
    "        ).rename(columns={'secs_elapsed':'total_time', 'device_type':'unique_device_types', 'action': 'unique_actions'})\n",
    "    return df.merge(session_agg, left_on='id', right_on='user_id', how='left')\n",
    "\n",
    "def nan_destroyer(df_input):\n",
    "    # funcation to remove nans from numerical fields, need to determine strategy\n",
    "    return none\n",
    "\n",
    "def feautre_selector(df_input):\n",
    "    # final function to remove IDs, repetitive features, and any features we do not want to be passed to our models\n",
    "    return none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preprocessor pipeline.\n",
    "def create_preprocessor_pipeline():\n",
    "\n",
    "    column_transformer = make_column_transformer(\n",
    "        (['gender',\n",
    "          'signup_method',\n",
    "          'signup_flow',\n",
    "          'language',\n",
    "          'affiliate_channel',\n",
    "          'affiliate_provider',\n",
    "          'first_affiliate_tracked',\n",
    "          'signup_app',\n",
    "          'first_device_type',\n",
    "          'first_browser',\n",
    "          'age_bucket',\n",
    "          'season'\n",
    "         ], OneHotEncoder(handle_unknown='ignore')),remainder='drop') # when we add in sessions features we will want to pass remainders\n",
    "    \n",
    "    preprocessor = make_pipeline(\n",
    "        FunctionTransformer(age_bucketer, validate=False),\n",
    "        FunctionTransformer(feature_creator, validate=False),\n",
    "        FunctionTransformer(clean_first_affiliate_tracked_nulls, validate=False),\n",
    "        column_transformer)\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py:739: DeprecationWarning: `make_column_transformer` now expects (transformer, columns) as input tuples instead of (columns, transformer). This has been introduced in v0.20.1. `make_column_transformer` will stop accepting the deprecated (columns, transformer) order in v0.22.\n",
      "  warnings.warn(message, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Stage: Data Preprocessor.\n",
    "preprocessor = create_preprocessor_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = balanced_train_data.iloc[:, 0:-1]\n",
    "balanced_labels = balanced_train_data.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will attempt to answer our research question through a variety of machine learning algorithms, as well as through an ensemble method combining several different approaches.  We will examine the efficacy of K-neighbors, Bernoulli Naive-Bayes, Random Forest, Logistic Regression, and XGBoost at predicting the destination country of new users, before combining several into an ensemble classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we start with a k-neighbors classifier.  Given the heavy skew of our data towards the NDF and US classes, we select a k-neighbors classifier to start, as the algorithm can perform well with unbalanced classes (CITATION) due to the model predicting off of nearby cases rather than generalized rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.523015369794201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00       928\n",
      "          CA       0.02      0.01      0.01      2719\n",
      "          DE       0.01      0.01      0.01      1862\n",
      "          ES       0.02      0.01      0.01      3038\n",
      "          FR       0.04      0.03      0.04      6805\n",
      "          GB       0.02      0.02      0.02      2563\n",
      "          IT       0.02      0.01      0.02      4688\n",
      "         NDF       0.75      0.65      0.70    136454\n",
      "          NL       0.00      0.00      0.00        24\n",
      "          PT       0.00      0.00      0.00        10\n",
      "          US       0.29      0.40      0.33     42705\n",
      "       other       0.01      0.06      0.02      1655\n",
      "\n",
      "   micro avg       0.52      0.52      0.52    203451\n",
      "   macro avg       0.10      0.10      0.10    203451\n",
      "weighted avg       0.57      0.52      0.54    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_neighbors':[3]}\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_gs = GridSearchCV(knn, params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, knn_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the model does perform fairly well on our dataset, and even returns the best Macro F1 scores our of any of our models when the number of neighbors is fairly low.  As the number of neighbors increases, the model's Macro F1 score declines while Weighted F1 improves, suggesting that there our imbalanced classes impose a tradeoff for the model.  The model predict time also slows as the number of neighbors increases, making further experimentation difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes offers a number of advantages for our dataset.  Our initial pipeline one-hot encodes all of our categorical features, a significant portion of our data is already in the binarized form required for Bernoulli Naive Bayes.  Furthermore, the model's use of Bayesian probabilities should help it avoid incorrectly predicting our less common classes. (REWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5676698566239536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00        16\n",
      "          CA       0.00      0.05      0.00        22\n",
      "          DE       0.00      0.00      0.00       144\n",
      "          ES       0.00      0.01      0.00       147\n",
      "          FR       0.00      0.02      0.00        58\n",
      "          GB       0.00      0.00      0.00        83\n",
      "          IT       0.00      0.03      0.00       152\n",
      "         NDF       0.67      0.70      0.69    113654\n",
      "          NL       0.00      0.00      0.00        18\n",
      "          PT       0.00      0.00      0.00         7\n",
      "          US       0.60      0.40      0.48     88602\n",
      "       other       0.00      0.05      0.01       548\n",
      "\n",
      "   micro avg       0.57      0.57      0.57    203451\n",
      "   macro avg       0.11      0.10      0.10    203451\n",
      "weighted avg       0.64      0.57      0.59    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'alpha': [1.05, .1]}\n",
    "bnb = BernoulliNB()\n",
    "bnb_gs = GridSearchCV(bnb, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, bnb_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.05}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the Laplace smoothing parameter (alpha), we are able to generate a model with a reasonably high accuracy and weighted F1 score, however, the model rarely predicts outside of our two main classes, NDF and US.  This results in a reduced macro F1 score.  This is likely because of the rarity of the other classes. (ADD MORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forst Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our next model, we choose a random forest classifier.  There is extensive literature on the history of decision-tree based classifiers for purchase prediction(CITATIONS NEEDED), and we examine the efficacy of this below. (ADD REASONS FOR EFFICACY IN OTHER CASES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4634482012867963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.01      0.00      0.00      2452\n",
      "          CA       0.02      0.01      0.01      6347\n",
      "          DE       0.01      0.01      0.01      1798\n",
      "          ES       0.02      0.01      0.01      4858\n",
      "          FR       0.03      0.02      0.03      5246\n",
      "          GB       0.02      0.01      0.02      3506\n",
      "          IT       0.03      0.01      0.02      6082\n",
      "         NDF       0.60      0.66      0.63    106768\n",
      "          NL       0.01      0.00      0.01      2964\n",
      "          PT       0.00      0.00      0.00       983\n",
      "          US       0.38      0.43      0.40     52595\n",
      "       other       0.06      0.06      0.06      9852\n",
      "\n",
      "   micro avg       0.46      0.46      0.46    203451\n",
      "   macro avg       0.10      0.10      0.10    203451\n",
      "weighted avg       0.42      0.46      0.44    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'n_estimators':[300], 'max_depth':[5,10,20]}\n",
    "rf = RandomForestClassifier(n_jobs = -1, class_weight = 'balanced')\n",
    "rf_gs = GridSearchCV(rf, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, rf_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20, 'n_estimators': 300}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In acccuracy, weighted F1, and macro F1 the random forest classifier performs worse than our other classifiers above.  Even after optimizing for the number of extimators and tree depth, the model still performs somewhat worse.  Unlike our two prior examples, this model predicts the uncommon classes at a much higher rate, but is inaccurate at doing so, as evidenced by the class-level F1s.  This could be due to the model \"overlearning\" feature combinations in the training phase, building out decision tree branches that ultimately do not generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5527964964536916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.07      0.00      0.01      7144\n",
      "          CA       0.03      0.01      0.02      3861\n",
      "          DE       0.01      0.00      0.00      1755\n",
      "          ES       0.03      0.02      0.02      2750\n",
      "          FR       0.00      0.02      0.00        92\n",
      "          GB       0.01      0.01      0.01       890\n",
      "          IT       0.01      0.02      0.01       819\n",
      "         NDF       0.78      0.69      0.74    134627\n",
      "          NL       0.03      0.01      0.01      3667\n",
      "          PT       0.03      0.00      0.00      7473\n",
      "          US       0.32      0.47      0.38     40287\n",
      "       other       0.00      0.15      0.00        86\n",
      "\n",
      "   micro avg       0.55      0.55      0.55    203451\n",
      "   macro avg       0.11      0.12      0.10    203451\n",
      "weighted avg       0.59      0.55      0.56    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'C':[.001,.01, .1, 1, 10, 100, .007], 'penalty':['l1', 'l2']}\n",
    "lr = LogisticRegression(class_weight = 'balanced', tol=.01)\n",
    "lr_gs = GridSearchCV(lr, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, lr_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.007, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6335333815021799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.00      0.00      0.00         0\n",
      "          CA       0.00      0.00      0.00         0\n",
      "          DE       0.00      0.00      0.00         0\n",
      "          ES       0.00      0.00      0.00         0\n",
      "          FR       0.00      0.00      0.00         0\n",
      "          GB       0.00      0.00      0.00         0\n",
      "          IT       0.00      0.00      0.00         0\n",
      "         NDF       0.85      0.69      0.76    146802\n",
      "          NL       0.00      0.00      0.00         0\n",
      "          PT       0.00      0.00      0.00         0\n",
      "          US       0.47      0.49      0.48     56649\n",
      "       other       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.63      0.63      0.63    203451\n",
      "   macro avg       0.11      0.10      0.10    203451\n",
      "weighted avg       0.74      0.63      0.68    203451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cventura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#params={'reg_lambda':[100]}\n",
    "params={'booster':['gbtree', 'gblinear','dart']}\n",
    "xgb = XGBClassifier(class_weight = 'balanced', nthread=-1)\n",
    "xgb_gs = GridSearchCV(xgb, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, xgb_gs)\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "bnb = BernoulliNB(alpha=1.05)\n",
    "rf = RandomForestClassifier(n_jobs = -1, n_estimators=300, max_depth=10, class_weight='balanced')\n",
    "lr = LogisticRegression()#C=.007, class_weight='balanced')\n",
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "vc = VotingClassifier(estimators = [('bnb', bnb),\n",
    "                                    ('rf', rf),\n",
    "                                    ('lr', lr),\n",
    "                                    ('xgb', xgb)], voting='hard')\n",
    "\n",
    "pipeline = make_pipeline(preprocessor, vc)\n",
    "\n",
    "final_model = cross_validate(pipeline, train_data, train_labels,\n",
    "                      scoring=[\"f1_weighted\"],\n",
    "                      return_train_score=True, cv=3, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Score: 0.6306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <th>train_f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.050808</td>\n",
       "      <td>1.058576</td>\n",
       "      <td>0.573121</td>\n",
       "      <td>0.603624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.908152</td>\n",
       "      <td>1.053457</td>\n",
       "      <td>0.585729</td>\n",
       "      <td>0.596188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.193894</td>\n",
       "      <td>0.947466</td>\n",
       "      <td>0.574207</td>\n",
       "      <td>0.600389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_f1_weighted  train_f1_weighted\n",
       "0  6.050808    1.058576          0.573121           0.603624\n",
       "1  5.908152    1.053457          0.585729           0.596188\n",
       "2  6.193894    0.947466          0.574207           0.600389"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show accuracy results.\n",
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "score = pipeline.score(dev_data, dev_labels.values.ravel())\n",
    "print(\"Pipeline Score: %.4f\" %(score))\n",
    "display(pd.DataFrame(final_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.29775719952224367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AU       0.19      0.01      0.01     14840\n",
      "          CA       0.17      0.01      0.03     15641\n",
      "          DE       0.14      0.01      0.02     11540\n",
      "          ES       0.13      0.02      0.03     16542\n",
      "          FR       0.13      0.04      0.06     18262\n",
      "          GB       0.11      0.02      0.03     14730\n",
      "          IT       0.07      0.02      0.03     10079\n",
      "         NDF       0.45      0.78      0.57     68193\n",
      "          NL       0.14      0.01      0.02     11078\n",
      "          PT       0.15      0.00      0.01      6301\n",
      "          US       0.08      0.45      0.13     10608\n",
      "       other       0.04      0.07      0.05      5637\n",
      "\n",
      "   micro avg       0.30      0.30      0.30    203451\n",
      "   macro avg       0.15      0.12      0.08    203451\n",
      "weighted avg       0.24      0.30      0.22    203451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(train_data, train_labels.values.ravel())\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "print('Accuracy: ',accuracy_score(dev_pred, dev_labels.values.ravel()))\n",
    "print(classification_report(dev_pred, dev_labels.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRITTEN: kaggle_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for test data to submit to Kaggle for scoring.\n",
    "predictions = pipeline.predict(test_data)\n",
    "\n",
    "# Save to csv\n",
    "final_csv = 'kaggle_submission.csv'\n",
    "predictions_pd = pd.DataFrame(data=predictions, columns=['country'])\n",
    "test_result = pd.concat([test_data['id'], predictions_pd], axis=1, sort=False)\n",
    "test_result.to_csv(final_csv, index=False)\n",
    "print(\"WRITTEN: %s\" %(final_csv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
