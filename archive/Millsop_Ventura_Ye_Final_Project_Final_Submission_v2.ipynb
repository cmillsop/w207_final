{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_gender_bkts.csv\n",
      "countries.csv\n",
      "sample_submission_NDF.csv\n",
      "sessions.csv\n",
      "test_users.csv\n",
      "train_users_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Import data files from Kaggle.\n",
    "DATA_PATH = './data/extracted'\n",
    "dfs_raw = {}\n",
    "dfs = {}\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        dfs_raw[file.split('.')[0]] = pd.read_csv(f'{DATA_PATH}/{file}')\n",
    "        dfs = dfs_raw.copy()\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into data and labels (panda dataframes).\n",
    "#reduced this to 10k / 213k since it was taking forever to even test anything\n",
    "#train_data   = dfs[\"train_users_2\"][:10000].iloc[:, 0:-1] #we should randomize since accounts are in chronological order\n",
    "#train_labels = dfs[\"train_users_2\"][:10000][\"country_destination\"].ravel()\n",
    "\n",
    "# Set train/dev split to 0.04685/0.95315 to give train size of 10k.  0.04685 = 10000/213451\n",
    "test_size = 0.95315\n",
    "\n",
    "# Set train/dev split to 0.5/0.5 to give train size of 107k.\n",
    "test_size = 0.5\n",
    "\n",
    "# Use (train_test_split) to randomize train_users_2 before splitting into train/dev.\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(dfs[\"train_users_2\"], dfs[\"train_users_2\"].iloc[:, -1:], test_size=test_size, random_state=42)\n",
    "\n",
    "# Final test data for Kaggle submission.\n",
    "test_data = dfs[\"test_users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to bucket ages prior to one-hot encoding\n",
    "def age_bucketer(df_input):\n",
    "    df = df_input\n",
    "    df.loc[(pd.isnull(df.age), 'age_bucket')] = 'unknown'\n",
    "    df.loc[(pd.notnull(df.age), 'age_bucket')] = pd.cut(df['age'],\n",
    "                                                        [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94,99,10000],\n",
    "                                                        labels=['0-4', '5-9', '10-14','15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49',\n",
    "                                                                '50-54', '55-59','60-64', '65-69','70-74','75-79','80-84','85-89','90-94','95-99','100+'],\n",
    "                                                        include_lowest=True)\n",
    "    return df.drop(['age'], axis=1)\n",
    "\n",
    "#Since NaN's in categorical data will cause issues with our pipeline we will replace that with \"unknown\".\n",
    "def clean_first_affiliate_tracked_nulls(df_input):\n",
    "    df_input['first_affiliate_tracked'] = df_input['first_affiliate_tracked'].fillna(\"unknown\", inplace=False)\n",
    "    return df_input\n",
    "\n",
    "#Add month and year features\n",
    "def feature_creator (df_input):\n",
    "    df = df_input\n",
    "    df['first_active_date'] = pd.to_datetime(df.timestamp_first_active,format='%Y%m%d%H%M%S')\n",
    "    df['year_first_active'] = df['first_active_date'].dt.year\n",
    "    df['month_first_active'] = df['first_active_date'].dt.month\n",
    "    return df.drop(['first_active_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preprocessor pipeline.\n",
    "def create_preprocessor_pipeline():\n",
    "\n",
    "    column_transformer = make_column_transformer(\n",
    "        (['gender',\n",
    "          'signup_method',\n",
    "          'signup_flow',\n",
    "          'language',\n",
    "          'affiliate_channel',\n",
    "          'affiliate_provider',\n",
    "          'first_affiliate_tracked',\n",
    "          'signup_app',\n",
    "          'first_device_type',\n",
    "          'first_browser',\n",
    "          'age_bucket'\n",
    "         ], OneHotEncoder(handle_unknown='ignore')),remainder='drop') # when we add in sessions features we will want to pass remainders\n",
    "    \n",
    "    preprocessor = make_pipeline(\n",
    "        FunctionTransformer(age_bucketer, validate=False),\n",
    "        FunctionTransformer(feature_creator, validate=False),\n",
    "        FunctionTransformer(clean_first_affiliate_tracked_nulls, validate=False),\n",
    "        column_transformer)\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sye\\AppData\\Local\\Continuum\\anaconda3\\envs\\mids-w203\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:739: DeprecationWarning: `make_column_transformer` now expects (transformer, columns) as input tuples instead of (columns, transformer). This has been introduced in v0.20.1. `make_column_transformer` will stop accepting the deprecated (columns, transformer) order in v0.22.\n",
      "  warnings.warn(message, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Stage: Data Preprocessor.\n",
    "preprocessor = create_preprocessor_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "vc = VotingClassifier(estimators = [('bnb', bnb), ('rf', rf), ('lr', lr)], voting='hard')\n",
    "\n",
    "pipeline = make_pipeline(preprocessor, vc)\n",
    "\n",
    "final_model = cross_validate(pipeline, train_data, train_labels,\n",
    "                      scoring=[\"f1_macro\"],\n",
    "                      return_train_score=True, cv=3, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>train_f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.702821</td>\n",
       "      <td>3.025173</td>\n",
       "      <td>0.102162</td>\n",
       "      <td>0.170734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.631841</td>\n",
       "      <td>2.453660</td>\n",
       "      <td>0.102409</td>\n",
       "      <td>0.168301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.976327</td>\n",
       "      <td>2.741166</td>\n",
       "      <td>0.101837</td>\n",
       "      <td>0.169084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_f1_macro  train_f1_macro\n",
       "0  47.702821    3.025173       0.102162        0.170734\n",
       "1  48.631841    2.453660       0.102409        0.168301\n",
       "2  47.976327    2.741166       0.101837        0.169084"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show accuracy results.\n",
    "display(pd.DataFrame(final_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7\n",
    "# Generate predictions for test data to submit to Kaggle for scoring.\n",
    "# predictions = clf.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
