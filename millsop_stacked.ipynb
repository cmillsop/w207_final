{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked ensemble attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I use ML-Ensemble to create a stacked ensemble using classifiers from the work described in the \"Final_Submission\" notebook.  A requirements.txt file has also been added that should be compatible with this notebook.\n",
    "\n",
    "Work towards the end of the notebook is unfinished.  At last attempt I had successfully created a stacked ensemble, however I was modifying the parameters to improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ml-ensemble.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_gender_bkts.csv\n",
      "countries.csv\n",
      "sample_submission_NDF.csv\n",
      "sessions.csv\n",
      "test_users.csv\n",
      "train_users_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Import data files from Kaggle.\n",
    "DATA_PATH = './data/extracted'\n",
    "dfs_raw = {}\n",
    "dfs = {}\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        dfs[file.split('.')[0]] = pd.read_csv(f'{DATA_PATH}/{file}')\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def transformToDatetime(series_input):\n",
    "    return pd.to_datetime(series_input,format='%Y%m%d%H%M%S', errors='coerce')\n",
    "\n",
    "#Function to bucket ages prior to one-hot encoding\n",
    "def getAgeBucket(df_input):\n",
    "    z = df_input.age\n",
    "    z = pd.to_numeric(df_input.age, errors='coerce')\n",
    "    return pd.cut(z,\n",
    "                    [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94,99,10000],\n",
    "                    labels=['0-4', '5-9', '10-14','15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49',\n",
    "                            '50-54', '55-59','60-64', '65-69','70-74','75-79','80-84','85-89','90-94','95-99','100+'],\n",
    "                    include_lowest=True)\n",
    "\n",
    "#Add month and year features\n",
    "def getYearFirstActive(df_input):\n",
    "    return pd.Series(transformToDatetime(df_input.timestamp_first_active).dt.year, index=df_input.index)\n",
    "\n",
    "def getMonthFirstActive(df_input):\n",
    "    return pd.Series(transformToDatetime(df_input.timestamp_first_active).dt.month, index=df_input.index)\n",
    "\n",
    "def getSeason(df_input):\n",
    "    season = pd.Series(transformToDatetime(df_input.timestamp_first_active).dt.month, index=df_input.index)\n",
    "    season[season.isin([12, 1, 2])] = 'Winter'\n",
    "    season[season.isin([3, 4, 5])] = 'Spring'\n",
    "    season[season.isin([6, 7, 8])] = 'Summer'\n",
    "    season[season.isin([9, 10, 11])] = 'Fall'\n",
    "    return season\n",
    "\n",
    "def getSessionActivityCount(df_input):\n",
    "    return dfs['sessions'].groupby(['user_id']).size().reset_index(name='counts').set_index('user_id')\n",
    "\n",
    "\n",
    "# Use this method to add features to datasets\n",
    "def add_features(df_input):\n",
    "    engineered_data = df_input\n",
    "    engineered_data['age_bucket'] = getAgeBucket(engineered_data)\n",
    "    engineered_data['first_active_year'] = getYearFirstActive(engineered_data)\n",
    "    engineered_data['first_active_month'] = getMonthFirstActive(engineered_data)\n",
    "    engineered_data['season'] = getSeason(engineered_data)\n",
    "    engineered_data['activity_count'] = getSessionActivityCount(engineered_data)\n",
    "    return engineered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Imputation\n",
    "  - Categorical: replace np.NaN with \"unknown\"\n",
    "  - Numerical: replace np.NaN with mean\n",
    "- Encoding\n",
    "  - Categorical: OneHotEncoder - convert categorical to multiple binary columns\n",
    "  - Numerical: StandardScaler - (value - mean) / unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preprocessor pipeline.\n",
    "\n",
    "def create_preprocessor_pipeline():\n",
    "    \n",
    "    categorical_columns_to_process = [\n",
    "        'gender',\n",
    "        'signup_method',\n",
    "        'signup_flow',\n",
    "        'language',\n",
    "        'affiliate_channel',\n",
    "        'affiliate_provider',\n",
    "        'first_affiliate_tracked',\n",
    "        'signup_app',\n",
    "        'first_device_type',\n",
    "        'first_browser',\n",
    "        'age_bucket',\n",
    "        'season',\n",
    "        'first_active_month',\n",
    "        'first_active_year'\n",
    "    ]\n",
    "    \n",
    "    numerical_columns_to_process = [\n",
    "        'activity_count'\n",
    "    ]\n",
    "    \n",
    "    return make_column_transformer(\n",
    "        (categorical_columns_to_process,\n",
    "         make_pipeline(\n",
    "             SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=\"unknown\"),\n",
    "             OneHotEncoder(handle_unknown='ignore')\n",
    "         )\n",
    "        ),\n",
    "        (numerical_columns_to_process,\n",
    "         make_pipeline(\n",
    "             SimpleImputer(missing_values=np.nan, strategy='mean'),\n",
    "             StandardScaler()\n",
    "         )\n",
    "        ),\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "preprocessor = create_preprocessor_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply feature engineering\n",
    "- Split data into training and development sets\n",
    "- Create a balanced training set to address imbalanced source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features to training and test data: stateful data mutations (ew)\n",
    "# attempted to use pipelines, column_transformer, function_transformer, feature_union\n",
    "# the features can't be arbitrarily included since function_transformer and column_transformer return ndarray's instead of dataframes\n",
    "\n",
    "all_train_data = add_features(dfs[\"train_users_2\"].iloc[:, 0:-1].copy().set_index('id'))\n",
    "all_train_labels = dfs[\"train_users_2\"].iloc[:, -1:]\n",
    "all_test_data = add_features(dfs[\"test_users\"].copy().set_index('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .5\n",
    "\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(all_train_labels.values.ravel())\n",
    "\n",
    "# Use (train_test_split) to randomize train_users_2 before splitting into train/dev.\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(all_train_data, encoded_labels, test_size=test_size, random_state=42)\n",
    "\n",
    "# Final test data for Kaggle submission.\n",
    "test_data = all_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_training_set():\n",
    "    countries = np.unique(encoded_labels)\n",
    "    z = train_data.copy()\n",
    "    z['dest'] = pd.Series(train_labels, index=z.index)\n",
    "    picks = round(len(train_data)/len(countries))\n",
    "    rx = []\n",
    "    for destination in countries:\n",
    "        options = z[z['dest'] == destination].index\n",
    "        ff = np.random.choice(options, picks)\n",
    "        rx.append(z.ix[ff])\n",
    "    \n",
    "    balanced_data = pd.concat(rx) \n",
    "    return balanced_data.iloc[:, 0:-1], balanced_data.iloc[:, -1:].values.ravel()\n",
    "\n",
    "balanced_train_data, balanced_train_labels = create_balanced_training_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain XGBoost baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a basic grid search for best boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33545715195922265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.01      0.01     10629\n",
      "           1       0.09      0.01      0.02      6198\n",
      "           2       0.18      0.01      0.02      9790\n",
      "           3       0.09      0.03      0.04      3941\n",
      "           4       0.06      0.04      0.05      3755\n",
      "           5       0.04      0.02      0.03      2307\n",
      "           6       0.10      0.03      0.04      5943\n",
      "           7       0.52      0.80      0.63     40541\n",
      "           8       0.09      0.01      0.01      5305\n",
      "           9       0.08      0.00      0.00      9003\n",
      "          10       0.07      0.46      0.13      4939\n",
      "          11       0.07      0.07      0.07      4375\n",
      "\n",
      "   micro avg       0.34      0.34      0.34    106726\n",
      "   macro avg       0.13      0.12      0.09    106726\n",
      "weighted avg       0.27      0.34      0.26    106726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={'booster':['gbtree', 'gblinear','dart']}\n",
    "xgb = XGBClassifier(nthread=-1)\n",
    "xgb_gs = GridSearchCV(xgb, params, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "pipeline = make_pipeline(preprocessor, xgb)\n",
    "pipeline.fit(balanced_train_data, balanced_train_labels)\n",
    "dev_pred = pipeline.predict(dev_data)\n",
    "accuracy = accuracy_score(dev_pred, dev_labels)\n",
    "print('Accuracy: ',accuracy)\n",
    "print(classification_report(dev_pred, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently configured to used the balanced dataset.\n",
    "\n",
    "- Layer 1: RandomForestClassifier, XGBClassifier\n",
    "- Layer 2: LogisticRegression\n",
    "\n",
    "- Ensemble #1 = don't propagate features from first layer to second layer.  don't propagate class probablities forward to second layer.  this is really just a proof of concept to get the stacking ensemble to work.  the logistic regression on the second layer only has the predicted classes from the first layer to work with, so we don't expect this to be any better than a voting classifier.\n",
    "- Ensemble #2 - it is the same as first except that it propagates everything forward (original data features, predicted classes, and probabilities of predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             done | 00:02:05\n",
      "Processing layer-2             done | 00:00:01\n",
      "Fit complete                        | 00:02:06\n",
      "\n",
      "Predicting 2 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:313: ParameterChangeWarning: Parameter value ('multi:softprob') has changed since model was fitted ('binary:logistic').\n",
      "  (lpar, rpar), ParameterChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1             done | 00:00:07\n",
      "Processing layer-2             done | 00:00:00\n",
      "Predict complete                    | 00:00:07\n",
      "Accuracy:  0.11075089481475929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.11      0.00      0.01      8830\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.06      0.00      0.01      6452\n",
      "         3.0       0.21      0.01      0.01     40785\n",
      "         4.0       0.00      0.00      0.00         0\n",
      "         5.0       0.11      0.01      0.03      9463\n",
      "         6.0       0.06      0.03      0.04      3586\n",
      "         7.0       0.09      0.47      0.16     12326\n",
      "         8.0       0.00      0.00      0.00         0\n",
      "         9.0       0.11      0.00      0.00     11408\n",
      "        10.0       0.18      0.40      0.24     13876\n",
      "        11.0       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.11      0.11      0.11    106726\n",
      "   macro avg       0.08      0.08      0.04    106726\n",
      "weighted avg       0.15      0.11      0.06    106726\n",
      "\n",
      "Fit data:\n",
      "                                   score-m  score-s   ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  randomforestclassifier       0.00     0.00  44.84  4.48  4.46  0.14\n",
      "layer-1  xgbclassifier                0.00     0.00  51.57  0.20  2.51  0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:313: ParameterChangeWarning: Parameter value ('multi:softprob') has changed since model was fitted ('binary:logistic').\n",
      "  (lpar, rpar), ParameterChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:226: MetricWarning: [randomforestclassifier.0.1] Could not score randomforestclassifier. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\")\n",
      "  (name, inst_name, exc), MetricWarning)\n",
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:226: MetricWarning: [xgbclassifier.0.2] Could not score xgbclassifier. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\")\n",
      "  (name, inst_name, exc), MetricWarning)\n",
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:226: MetricWarning: [randomforestclassifier.0.2] Could not score randomforestclassifier. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\")\n",
      "  (name, inst_name, exc), MetricWarning)\n",
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:226: MetricWarning: [xgbclassifier.0.1] Could not score xgbclassifier. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\")\n",
      "  (name, inst_name, exc), MetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done | 00:02:08\n",
      "Processing layer-2             done | 00:00:45\n",
      "Fit complete                        | 00:02:55\n",
      "\n",
      "Predicting 2 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christmi\\data-sci\\common\\lib\\site-packages\\mlens\\parallel\\_base_functions.py:313: ParameterChangeWarning: Parameter value ('multi:softprob') has changed since model was fitted ('binary:logistic').\n",
      "  (lpar, rpar), ParameterChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1             done | 00:00:07\n",
      "Processing layer-2             done | 00:00:00\n",
      "Predict complete                    | 00:00:09\n",
      "Accuracy:  0.33034124768097745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.05      0.00      0.01      3425\n",
      "         1.0       0.02      0.01      0.01      1561\n",
      "         2.0       0.06      0.01      0.02      3168\n",
      "         3.0       0.02      0.01      0.02      1207\n",
      "         4.0       0.01      0.05      0.02       604\n",
      "         5.0       0.02      0.02      0.02      1376\n",
      "         6.0       0.12      0.03      0.04      6516\n",
      "         7.0       0.49      0.80      0.61     38102\n",
      "         8.0       0.21      0.01      0.01     10988\n",
      "         9.0       0.28      0.00      0.00     25950\n",
      "        10.0       0.13      0.47      0.20      8647\n",
      "        11.0       0.09      0.08      0.09      5182\n",
      "\n",
      "   micro avg       0.33      0.33      0.33    106726\n",
      "   macro avg       0.13      0.12      0.09    106726\n",
      "weighted avg       0.29      0.33      0.24    106726\n",
      "\n",
      "Fit data:\n",
      "                                    ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  randomforestclassifier    45.74  4.10  4.26  0.06\n",
      "layer-1  xgbclassifier             52.29  0.52  2.13  0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stacking(propagate=None, proba=None):\n",
    "    seed = 142\n",
    "    \n",
    "    # --- Build ---\n",
    "    # Passing a scoring function will create cv scores during fitting\n",
    "    # the scorer should be a simple function accepting to vectors and returning a scalar\n",
    "    ensemble = SuperLearner(scorer=accuracy_score, random_state=seed, verbose=2)\n",
    "    pf = False\n",
    "    if propagate:\n",
    "        pf = list(range(preprocessor.fit_transform(balanced_train_data).shape[1]))\n",
    "    # Build the first layer\n",
    "    ensemble.add([\n",
    "        RandomForestClassifier(random_state=seed,n_estimators=300, max_depth=10),\n",
    "        XGBClassifier()\n",
    "    ],\n",
    "        #preprocessing=[preprocessor],\n",
    "        propagate_features=pf,\n",
    "        proba=proba)\n",
    "\n",
    "    # Attach the final meta estimator\n",
    "    ensemble.add_meta(LogisticRegression())\n",
    "\n",
    "    # --- Use ---\n",
    "\n",
    "    # Fit ensemble\n",
    "    pipeline = make_pipeline(preprocessor, ensemble)\n",
    "    pipeline.fit(balanced_train_data, balanced_train_labels)\n",
    "    \n",
    "    dev_pred = pipeline.predict(dev_data)\n",
    "    accuracy = accuracy_score(dev_pred, dev_labels)\n",
    "\n",
    "    #ensemble.fit(train_data, train_labels)\n",
    "\n",
    "    # Predict\n",
    "    #preds = ensemble.predict(dev_data)\n",
    "    print('Accuracy: ',accuracy)\n",
    "    print(classification_report(dev_pred, dev_labels))\n",
    "    print(\"Fit data:\\n%r\" % ensemble.data)\n",
    "    #print(\"Prediction score: %.3f\" % accuracy_score(preds, dev_labels))\n",
    "\n",
    "stacking(False,False)\n",
    "stacking(True,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-sci-common",
   "language": "python",
   "name": "data-sci-common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
